{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Model Drift Workshop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO ADD CREDITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import ClassificationPreset, DataDriftPreset, DataQualityPreset, TargetDriftPreset\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import NoTargetPerformanceTestPreset, DataQualityTestPreset, DataStabilityTestPreset, DataDriftTestPreset, MulticlassClassificationTestPreset\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import datasets, ensemble, model_selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop setup: datasets and models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification model and Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = datasets.load_iris(as_frame=\"auto\")\n",
    "iris = iris_data.frame\n",
    "\n",
    "iris_ref, iris_cur = model_selection.train_test_split(iris, test_size=0.3)\n",
    "\n",
    "clas_model = ensemble.RandomForestClassifier(random_state=42, n_estimators=3)\n",
    "clas_model.fit(iris_ref[iris_data.feature_names], iris_ref.target)\n",
    "\n",
    "iris_ref[\"prediction\"] = clas_model.predict(iris_ref[iris_data.feature_names])\n",
    "iris_cur[\"prediction\"] = clas_model.predict(iris_cur[iris_data.feature_names])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model and California housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = datasets.fetch_california_housing(as_frame=\"auto\")\n",
    "housing = housing_data.frame\n",
    "\n",
    "housing.rename(columns={\"MedHouseVal\": \"target\"}, inplace=True)\n",
    "numerical_features_reg = [\n",
    "    \"MedInc\",\n",
    "    \"HouseAge\",\n",
    "    \"AveRooms\",\n",
    "    \"AveBedrms\",\n",
    "    \"Population\",\n",
    "    \"AveOccup\",\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "]\n",
    "categorical_features_reg = []\n",
    "features_reg = numerical_features_reg\n",
    "\n",
    "housing_ref = housing.sample(n=5000, replace=False)\n",
    "housing_cur = housing.sample(n=5000, replace=False)\n",
    "\n",
    "reg_model = ensemble.RandomForestRegressor(random_state=42)\n",
    "reg_model.fit(housing_ref[features_reg], housing_ref.target)\n",
    "\n",
    "housing_ref[\"prediction\"] = reg_model.predict(housing_ref[features_reg])\n",
    "housing_cur[\"prediction\"] = reg_model.predict(housing_cur[features_reg])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance\n",
    "\n",
    "Goal:\n",
    "* understand a Column Mapping concept\n",
    "* try out an Exercise 1 for a better Column Mapping concept understanding\n",
    "* explore a pre-built report for the Classification model performance\n",
    "* explore a pre-built report for the Regression model performance during the Exercise 2 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column mapping\n",
    "\n",
    "Evidently expects a certain dataset structure and input column names. You can specify any differences by creating a ColumnMapping object. It works the same way for test suites and reports. If the `column_mapping` is not specified or set as `None`, Evidently will use the default mapping strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_column_mapping = ColumnMapping()\n",
    "\n",
    "clas_column_mapping.numerical_features = [\n",
    "    \"sepal length (cm)\",\n",
    "    \"sepal width (cm)\",\n",
    "    \"petal length (cm)\",\n",
    "    \"petal width (cm)\",\n",
    "]\n",
    "\n",
    "clas_column_mapping.target = \"target\"\n",
    "clas_column_mapping.target_names = [\"Setosa\", \"Versicolour\", \"Virginica\"]\n",
    "clas_column_mapping.prediction = \"prediction\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "Map columns for the regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore a pre-built report for the Classification model performance\n",
    "\n",
    "**Classification Performance report** evaluates the quality of a classification model. It works both for binary and multi-class classification. \n",
    "\n",
    "NOTE: There is a separate report for a probabilistic classification.\n",
    "\n",
    "This report can be generated for a single model, or as a comparison. You can contrast your current production model performance against the past or an alternative model.\n",
    "\n",
    "#### When to use the report\n",
    "\n",
    "1. To analyze the results of the model test. You can explore the results of an online or offline test and contrast it to the performance in training. Though this is not the primary use case, you can use this report to compare the model performance in an A/B test, or during a shadow model deployment.\n",
    "\n",
    "2. To generate regular reports on the performance of a production model. You can run this report as a regular job (e.g. weekly or at every batch model run) to analyze its performance and share it with other stakeholders.\n",
    "\n",
    "3. To analyze the model performance on the slices of data. By manipulating the input data frame, you can explore how the model performs on different data segments (e.g. users from a specific region).\n",
    "\n",
    "4. To trigger or decide on the model retraining. You can use this report to check if your performance is below the threshold to initiate a model update and evaluate if retraining is likely to improve performance.\n",
    "\n",
    "5. To debug or improve model performance. You can use the Classification Quality table to identify underperforming segments and decide on ways to address them.\n",
    "\n",
    "To run this report, you need to have **input features**, and **both target and prediction** columns available. You can use both **numerical labels** like \"0\", \"1\", \"2\" or **class names** like \"virginica\", \"setoza\", \"versicolor\" inside the target and prediction columns. The labels should be the same for the target and predictions. NOTE: Column order in Binary Classification. For binary classification, class order matters. The tool expects that the target (so-called positive) class is the first in the column_mapping['prediction'] list.\n",
    "\n",
    "To generate a comparative report, you will need the **two** datasets. **The reference dataset** serves as a benchmark. We analyze the change by comparing **the current production data** to **the reference data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_performance_report = Report(\n",
    "    metrics=[\n",
    "        ClassificationPreset(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "classification_performance_report.run(\n",
    "    reference_data=iris_ref, current_data=iris_cur, column_mapping=clas_column_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_performance_report.show(mode=\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_performance_report.save_html(\n",
    "    Path(\"reports\", \"clas_perf_report.html\")\n",
    ")\n",
    "classification_performance_report.save_json(\n",
    "    Path(\"reports\", \"class_perf_report.json\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it looks\n",
    "\n",
    "The report includes 5 components. All plots are interactive.\n",
    "\n",
    "1. Model Quality Summary Metrics\n",
    "We calculate a few standard model quality metrics: Accuracy, Precision, Recall, and F1-score. To support the model performance analysis, we also generate interactive visualizations. They help analyze where the model makes mistakes and come up with improvement ideas.\n",
    "\n",
    "2. Class Representation\n",
    "Shows the number of objects of each class.\n",
    "\n",
    "3. Confusion Matrix\n",
    "Visualizes the classification errors and their type.\n",
    "\n",
    "4. Quality Metrics by Class\n",
    "Shows the model quality metrics for the individual classes.\n",
    "\n",
    "5. Classification Quality by Feature.\n",
    "\n",
    "In this table, we show a number of plots for each feature. To expand the plots, click on the feature name. In the tab “ALL”, we plot the distribution of classes against the values of the feature. This is the “Target Behavior by Feature” plot from the Categorial Target Drift report. If you compare the two datasets, it visually shows the changes in the feature distribution and in the relationship between the values of the feature and the target. Then, for each class, we plot the distribution of the True Positive, True Negative, False Positive, and False Negative predictions alongside the values of the feature. It visualizes the regions where the model makes errors of each type and reveals the low-performance segments. This helps explore if a specific type of misclassification error is sensitive to the values of a given feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore a pre-built report for the Regression model performance\n",
    "\n",
    "**The Regression Performance report** evaluates the quality of a regression model. It can also compare it to the past performance of the same model, or the performance of an alternative model.\n",
    "\n",
    "#### When to use the report\n",
    "\n",
    "1. To analyze the results of the model test. You can explore the results of an online or offline test and contrast it to the performance in training. Though this is not the primary use case, you can use this report to compare the model performance in an A/B test, or during a shadow model deployment.\n",
    "\n",
    "2. To generate regular reports on the performance of a production model. You can run this report as a regular job (e.g. weekly or at every batch model run) to analyze its performance and share it with other stakeholders.\n",
    "\n",
    "3. To analyze the model performance on the slices of data. By manipulating the input data frame, you can explore how the model performs on different data segments (e.g. users from a specific region).\n",
    "\n",
    "4. To trigger or decide on the model retraining. You can use this report to check if your performance is below the threshold to initiate a model update and evaluate if retraining is likely to improve performance.\n",
    "\n",
    "5. To debug or improve model performance by identifying areas of high error. You can use the Error Bias table to identify the groups that contribute way more to the total error, or where the model under- or over-estimates the target function.\n",
    "\n",
    "To run this report, you need to have **input features**, and **both target and prediction columns** available. To generate a comparative report, you will need **two** datasets. **The reference dataset** serves as a benchmark. We analyze the change by comparing **the current production data** to **the reference data**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Create a Regression model performance report for the regression model, run it, show it and save it as html and json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the report and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the report as html and json\n",
    "\n",
    "# \"reports\", \"reg_perf_report.html\"\n",
    "# \"reports\", \"reg_perf_report.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How it looks\n",
    "\n",
    "The report includes 12 components. All plots are interactive.\n",
    "\n",
    "1. Model Quality Summary Metrics\n",
    "We calculate a few standard model quality metrics: Mean Error (ME), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE). For each quality metric, we also show one standard deviation of its value (in brackets) to estimate the stability of the performance. Next, we generate a set of plots. They help analyze where the model makes mistakes and come up with improvement ideas.\n",
    "\n",
    "2. Predicted vs Actual\n",
    "Predicted versus actual values in a scatter plot.\n",
    "\n",
    "3. Predicted vs Actual in Time\n",
    "Predicted and Actual values over time or by index, if no datetime is provided.\n",
    "\n",
    "4. Error (Predicted - Actual)\n",
    "Model error values over time or by index, if no datetime is provided.\n",
    "\n",
    "5. Absolute Percentage Error\n",
    "Absolute percentage error values over time or by index, if no datetime is provided.\n",
    "\n",
    "6. Error Distribution\n",
    "Distribution of the model error values.\n",
    "\n",
    "7. Error Normality\n",
    "Quantile-quantile plot (Q-Q plot) to estimate value normality. Next, we explore in detail the two segments in the dataset: 5% of predictions with the highest negative and positive errors. We refer to them as \"underestimation\" and \"overestimation\" groups. We refer to the rest of the predictions as \"majority\".\n",
    "\n",
    "8. Mean Error per Group\n",
    "We show a summary of the model quality metrics for each of the two groups: mean Error (ME), Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "9. Predicted vs Actual per Group\n",
    "We plot the predictions, coloring them by the group they belong to. It visualizes the regions where the model underestimates and overestimates the target function.\n",
    "\n",
    "10. Error Bias: Mean/Most Common Feature Value per Group\n",
    "This table helps quickly see the differences in feature values between the 3 groups:\n",
    "\n",
    "OVER (top-5% of predictions with overestimation)\n",
    "UNDER (top-5% of the predictions with underestimation)\n",
    "MAJORITY (the rest 90%)\n",
    "For the numerical features, it shows the mean value per group. For the categorical features, it shows the most common value. If you have two datasets, the table displays the values for both REF (reference) and CURR (current). If you observe a large difference between the groups, it means that the model error is sensitive to the values of a given feature. To search for cases like this, you can sort the table using the column \"Range(%)\". It increases when either or both of the \"extreme\" groups are different from the majority.\n",
    "\n",
    "11. Error Bias per Feature\n",
    "For each feature, we show a histogram to visualize the distribution of its values in the segments with extreme errors and in the rest of the data. You can visually explore if there is a relationship between the high error and the values of a given feature.\n",
    "\n",
    "12. Predicted vs Actual per Feature\n",
    "For each feature, we also show the Predicted vs Actual scatterplot. We use colors to show the distribution of the values of a given feature. It helps visually detect and explore underperforming segments which might be sensitive to the values of the given feature."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the drift"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "* get familiarity with pre-built Data Quality, Data Drift and Target Drift reports for the classification model\n",
    "* explore pre-built Data Quality, Data Drift abd Target Drift reports for the regression model during the Exercise 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Drift report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Drift report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Create a Data Quality report, Data Drift report, Target Drift report for the regression model, run them, show them and save them as html and json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all 3 reports and run them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the reports as html and json\n",
    "\n",
    "# \"reports\", \"reg_drift_report.html\"\n",
    "# \"reports\", \"reg_drift_report.json\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-based monitoring\n",
    "\n",
    "Goal:\n",
    "* lorem ipsum\n",
    "* lorem ipsum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7 (main, Sep 14 2022, 22:38:23) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "02e39585b4b4b1897ed0125bd838291ce52131bea27edd2a1bc31e43ed58f171"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
